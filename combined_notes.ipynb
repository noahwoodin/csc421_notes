{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CSC 421 - Agents \n",
    "\n",
    "### Instructor: George Tzanetakis \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents \n",
    "\n",
    "\n",
    "**EMPHASIS**: Agents as a unifying concept of thinking about AI and software \n",
    "\n",
    "\n",
    "During this lecture we will cover the following topics: \n",
    "\n",
    "1. Agents \n",
    "2. Performance, environments, actuators, sensors \n",
    "3. Agent architectures \n",
    "7. Learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## WORKPLAN \n",
    "\n",
    "The section number is based on the 4th edition of the AIMA textbook and is the suggested\n",
    "reading for this week. Each list entry provides just the additional sections. For example the Expected reading include the sections listed under Basic as well as the sections listed under Expected. Some additional readings are suggested for Advanced. \n",
    "\n",
    "1. Basic: Sections **2.1**, **2.3**, **2.4** and Summary  \n",
    "2. Expected: **2.2**\n",
    "3. Advanced: Bibligraphical and historical notes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents and Environments  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agents** perceive their **environment** through **sensors** and act upon that environment through **actuators**. \n",
    "\n",
    "\n",
    "Terminology: \n",
    "\n",
    "1. Percept \n",
    "2. Percept sequence \n",
    "3. Agent function (abstract mathematical distribution, in many cases infinite tabulation) \n",
    "4. Agent program (concrete implementation running on a physical system) \n",
    "\n",
    "What makes an agent effective, good, intelligent ? \n",
    "\n",
    "\n",
    "Any area of engineering can be viewed through the lenses of agents. What makes AI unique is the significant computational resources that can be employed by the agent and the non-trivial decision making that the task environment requires. In fact one could possibly define AI agents as systems that can not be developed using traditional engineering approaches.\n",
    "\n",
    "<img src=\"images/aima_simple_agent.png\" width=\"70%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEAS Description of Agent \n",
    "\n",
    "1. **Performance** a way to measure how the agent is doing \n",
    "2. **Environment** essential the problems or worlds in which the agent needs to operate \n",
    "3. **Actuators** are the different ways the agent can be interact with the environment as well as possibly its own\n",
    "operation. They receive **actions** that encode the information needed for them to operate. \n",
    "4. **Sensors** are the ways the agent can acquire information about the environment it is operating as well as possibly \n",
    "information about its own operation. The information they acquire is represented as **percepts**.f\n",
    "\n",
    "Let's consider some examples - what are the possible percepts, environments, sensors and actuators for these \n",
    "agents: \n",
    "\n",
    "1.  Human \n",
    "2.  Robot \n",
    "3.  Vacuum cleaner world \n",
    "4.  Single chess piece valid chessboard moves \n",
    "5.  Self-driving car \n",
    "6.  Ant \n",
    "7.  NPC in game \n",
    "8.  Chess playing program \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK ENVIRONMENTS \n",
    "\n",
    "Specifying the task environment (essentially the problem to which rational agents are the solutions): \n",
    "\n",
    "\n",
    "1. Performance \n",
    "2. Environment \n",
    "3. Actuators \n",
    "4. Sensors \n",
    "\n",
    "\n",
    "Properties of task environmets (for each one think of examples or consider the examples mentioned above): \n",
    "\n",
    "1. Fully observable vs partially observable \n",
    "2. Single-agent vs multiagent \n",
    "    1. Competitive multiagent (chess) vs co-operative multiagent (self-driving cars avoiding collisions)\n",
    "3. Deterministic vs nondeterministic \n",
    "\n",
    "\n",
    "**Agent = architecture + program** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of Agents \n",
    "\n",
    "1. **Reflex agents** \n",
    "\n",
    "Reflex agents simply act based on the current percept ignoring the rest of the percept history. They can \n",
    "operate using simple condition-action rules. Humans have many such rules that are typically used when fast action \n",
    "in response to a stimulus is required. \n",
    "\n",
    "<img src=\"images/aima_simple_agent.png\" width=\"70%\"/>\n",
    "\n",
    "2. **Model-based reflex agents**\n",
    "\n",
    "The most effective way for an agent to deal with a partially observable environment is to maintain \n",
    "some kind of internal representation (model) keeping track of the parts of the world that it can not \n",
    "perceive. This model needs to be updated based on knowledge about how the world changes independently of the \n",
    "agent as well as about how the agent's own actions can affect the world. \n",
    "\n",
    "<img src=\"images/aima_model_agent.png\" width=\"70%\"/>\n",
    "\n",
    "\n",
    "\n",
    "5. **Goal-based agents** \n",
    "\n",
    "Knowing something about the current state of the world is not always sufficient in order to decide what to do. There are many situations in which the agent nneds to have some sort of **goal** information that describes situations that are desirable. Goal-based agents are fundementally different than reflex agents as they consider the future. We we will be looking at **Search** (in detail) and **Planning** which are two research areas of AI that focus on finding action sequences for agents to achieve specific goals. Goal agents are more flexible than reflex-agents but in general tend to be more computationally demanding and therefore slower as they need to consider how actions create multiple possible \"futures\" and determine if these **futures** meat specific **goals**. \n",
    "\n",
    "<img src=\"images/aima_goal_agent.png\" width=\"70%\"/>\n",
    "\n",
    "\n",
    "\n",
    "6. **Utility-based agents**\n",
    "   \n",
    "Goals alone are not enough. Utility is an internal representation of the performance measure. You can think of it as a \"happiness\" measure for the agent. Utility has the same relationship with an external performance measure that the internal world representation of an agent has with the actual world/environment it operates. Utility can assist in two situations in which goal-based agents have a hard time: 1) when there are conflicting goals (for example speed and safety) 2) when there are multiple goals that the agent can aim for but none of which can achieved with certainty. Because uncertainty is always present in typical real-world situations requiring rational/intelligent behavior technically speaking a utility-based agent chooses the action that maximizes the **expected utility**. \n",
    "\n",
    "<img src=\"images/aima_utility_agent.png\" width=\"70%\"/>\n",
    "\n",
    "Discussion Examples: \n",
    "NPC in a graph-based text adventure \n",
    "Driving to the airport \n",
    "\n",
    "\n",
    "**IMPORTANT: CLEAR SEPARATION OF AGENT AND ENVIRONMENT WHEN DOING SIMULATIONS**\n",
    "\n",
    "\n",
    "### LEARNING AGENTS \n",
    "\n",
    "All types of agent architectures can benefit from learning. Learning occurs when the peformance measure with which we measure how the agent is doing improves through \"experience\" i.e repeated operation in an environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World/Environment Representations \n",
    "\n",
    "**Imortant note:** The environment representation is different than the environment and it consists of what \n",
    "the agent \"knows\" about it so in most cases it does not contain all the information in the environment. \n",
    "\n",
    "Atomic representation, factored representation, structured representation, distributed representation \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/aima_environment_representations.png\" width=\"70%\"/>\n",
    "\n",
    "\n",
    "* Algorithms for search and game-playing, hidden markov modes, and markov decision processes work with atomic representations. \n",
    "* Constraint satisfaction problems, propositional logic, bayesian networks, and machine learning algorithms frequently work with factored representatinos. \n",
    "* Relational databases, first-order logic, natural language understanding and knowledge-based learning operate on structured representation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code sketch of a reflex agent. The set of rules (condition-action pairs) \n",
    "# can remain static through the execution or can be modified if the agent is capable of learning. \n",
    "\n",
    "def simple_rule_agent(percept, rules): \n",
    "    state = interpet_input(percept)\n",
    "    rule  = rule_match(state,rules) \n",
    "    action = rule.action() \n",
    "    return action\n",
    "\n",
    "# code sketch of a model-based reflex agent \n",
    "# state: the agen'ts current conception of the world state \n",
    "# model: a description of how the next state depends on the current state and action \n",
    "def model_agent(percept, rules, state, model): \n",
    "    state = update_state(state, action, percept, model) \n",
    "    rule = rule_match(state, rules) \n",
    "    action = rule.action() \n",
    "    return action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC 421 - Constraint Satisfaction Problems\n",
    "\n",
    "### Instructor: George Tzanetakis \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Problems can be solved by searching the state space. The state space is a representation of a problem where the nodes are states and the edges between them are actions. Domain-specific heuristics can be used to estimate the cost of reaching the goakl from a given state to make searching for solutions more efficient. The state representation is treated as a black box. \n",
    "\n",
    "**Constraint Satisfaction Problems** are a specific type of problem in which the state representation is **factored**. The state in such problems can be represented as a set of **variables** each of which has a **value** from a particular **domain**. A problem is solved when each variable has a value that satisfies \n",
    "all the constraints that involve that variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Defining a CSP problem: \n",
    "\n",
    "* X is a set of variables {X1, X2, .. XN} \n",
    "* D is a set of domains {D1, ...., Dn} (one for each variable) \n",
    "* C is a set of constraints that specify allowable combinations of values \n",
    "\n",
    "Any contraint can be represented as an explicit set of tuples of values that satisfy the constraint or as a membership function (typically syntactic sugar). \n",
    "\n",
    "For example X1 and X2 with domain {1,2,3}. \n",
    "\n",
    "X1>X2 can be written as {(3,1),(3,2),(2,1)}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **consistent** assignment is an assignment of values to variables that does not violate any constraints. \n",
    "A **complete** assignment is an assignment of values to all variables. A **complete** and **consistent** assignment is called a **solution** to the CSP problem. A **partial** assignment is one that leaves some variables unassigned, and a **partial solution** is a partial assignment that is consistent. \n",
    "\n",
    "Solving a **CSP** problem is an NP-complete problem in general. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic example - map coloring "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/csp_australia.png\" width=\"60%\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SA != WA \n",
    "\n",
    "Full enumeration \n",
    "\n",
    "{(SA,WA): (red, green), (red, blue), (green, red), (green,blue), (blue, red), (blue, green)}\n",
    "\n",
    "Many possible solutions. For example: \n",
    "\n",
    "{WA=red, NT=green, Q=red, NSW=green, V=red, SA=blue, T=red} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CSP we can leverage information during search: \n",
    "\n",
    "* Partial assignment violates a constraint we can immediately discard further refinements of the partial assignment \n",
    "* We can see which variable violate a constraint and focus attention on the variables that matter \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical Sidenote 1 \n",
    "\n",
    "The 4-color conjecture states that any planar graph can be colored with four or fewer colors. \n",
    "A planar graph is a graph that can be drawn on a place without the edges crossing each other. \n",
    "\n",
    "\n",
    "Probably first made by Francis Guthrie, student of De Morgan in 1852. Despite efforts first proof in 1977 by Appel and Haken (with computer aid). This is a historic proof because it was the first widely accepted proof that was made by a computer system. \n",
    "\n",
    "\n",
    "<img src=\"images/four_color.png\" width=\"30%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical Sidenote 2 \n",
    "\n",
    "CSP algorithms were used in SketchPad by Ivan Sutherland in 1963. Forerunner of pointer/display interaction, CAD, etc. \n",
    "\n",
    "\n",
    "<img src=\"images/sketchpad.png\" width=\"50%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating rhythmic exercises at different levels of difficulty \n",
    "\n",
    "Joint work with Graham Percival, Torsten Anders, and George Tzanetakis\n",
    "\n",
    "\n",
    "<img src=\"images/rhythmic_exercises_csp1.png\" width=\"75%\"/>\n",
    "\n",
    "<img src=\"images/rhythmic_exercises_csp2.png\" width=\"75%\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive backtracking - start with an initial assignment (usually empty) and then select a variable and \n",
    "# assign it a value. If the current assignment is consistent with the constraints call recursively \n",
    "\n",
    "def recursive_backtracking(assignment, csp):\n",
    "    if isComplete(assignment):\n",
    "        return assignment\n",
    "    var = select_unassigned_variable(csp[\"VARIABLES\"], assignment)\n",
    "    for value in csp[\"DOMAINS\"]:\n",
    "        assignment[var] = value\n",
    "        if is_consistent(assignment, csp[\"CONSTRAINTS\"]):\n",
    "            result = recursive_backtracking(assignment, csp)\n",
    "            if result != \"FAILURE\":\n",
    "                return result\n",
    "        assignment[var] = None\n",
    "    return \"FAILURE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/recursive_backtracking_csp_australia.png\" width=\"75%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristics to use during backtracking search \n",
    "\n",
    "1. Minimum remaining values:  i.e choose the variable with the fewest legal values \n",
    "2. Degree heuristic: Tie-breaker among MRV variables - choose the one with the most constraints with remaining variables \n",
    "3. Least constraining variable: rules out the fewest choices for the neighboring variables in the constraint graph\n",
    "\n",
    "\n",
    "<img src=\"images/least_constraining_variable_csp.png\" width=\"75%\"/>\n",
    "\n",
    "\n",
    "4. Forward Checking: keep track of remaining legal values for unassigned variables \n",
    "\n",
    "<img src=\"images/forward_checking_csp.png\" width=\"75%\"/>\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "\n",
    "1. CSPs are a special kind of problem \n",
    "2. States defined by values of a fixed set of variables\n",
    "3. Goal test defined by constraints of variable values\n",
    "4. Back-tracking = depth-first search with one variable assigned per node\n",
    "5. Variable ordering and value selection heuristics can help significantly\n",
    "6. Forward checking prevents assignments that guarantee later failure \n",
    "7. Specific-constraint type and structure (for example trees) can lead to more efficient solvers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
